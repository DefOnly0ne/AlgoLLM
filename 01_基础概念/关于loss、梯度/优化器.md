# 学习笔记：梯度下降优化算法的演进之路

## 0. 问题的起点：为什么需要优化算法？

### 0.1 理想 vs 现实

假设损失函数的图像是一个**碗形**（凸函数），参数从随机初始化的位置开始优化：

```
起点 ●
    ╲  ╱      实际路径：左右震荡 ╲╱╲╱
     ╲╱               同时逐渐下降
      ╲  ╱    
       ╲╱     理想路径：直线下降 ↓
        ╲ ╱
         ╲╱
          ★
        碗底
      （最小值）
```

**核心矛盾**：
- **理想情况**：沿着直线直奔碗底（最小值）
- **实际情况**：在**水平方向**左右震荡，**竖直方向**逐渐逼近碗底

> **优化目标**：如果能把震荡抵消掉，收敛速度会大幅提升！

---

## 1. 动量梯度下降法（Momentum）

### 1.1 核心思想：利用"惯性"

**灵感**：把**下一步要移动的向量**和**过去移动的向量相加**，可以：
1. **抵消震荡**（正负方向的梯度互相抵消）
2. **加速逼近**（持续的方向会累积速度）

### 1.2 伪动量算法（有缺陷版本）

**公式**：

```math
V_t = V_{t-1} + \nabla_\theta L
```

```math
\theta = \theta - \eta \cdot V_t
```

- $V_t$ ：累积的"速度"向量
- $\nabla_\theta L$ ：当前梯度

**致命问题**：
> 每次都把**全额梯度累加**，导致 $V_t$ 无限累加，最终会**越过最小值**，无法停下！

### 1.3 真·动量算法（引入衰减因子）

**公式**：

```math
V_t = \beta \cdot V_{t-1} + \nabla_\theta L
```

```math
\theta = \theta - \eta \cdot V_t
```

- $\beta$ ：动量系数（通常取 **0.9**）
- 过去的动量会**按比例衰减**，避免无限累积

**数学本质**：**指数加权移动平均（Exponential Weighted Moving Average, EWMA）**

```math
V_t = \nabla_\theta L + 0.9 \cdot \nabla_\theta L_{t-1} + 0.9^2 \cdot \nabla_\theta L_{t-2} + \cdots
```

> **效果**：
> - 近期梯度权重大，远期梯度逐渐衰减
> - 震荡方向的梯度互相抵消
> - 持续方向的梯度不断累积

---

## 2. RMSprop：自适应学习率

### 2.1 新的问题：固定学习率的弊端

动量法解决了"方向"问题，但**学习率 $\eta$ 是固定的**，存在两大问题：

1. **越接近碗底，更新幅度反而变大**
   - 原因：梯度虽然变小，但学习率不变
   - 后果：在最小值附近来回震荡，无法精确收敛

2. **不同方向使用同一学习率不合理**
   - 陡峭方向（梯度大）：应该小步慢走
   - 平缓方向（梯度小）：应该大步快走

### 2.2 核心思想：让"大变小，小变大"

**需要找到这样一个数**：
- 当梯度大时，**分母大** → 和分子抵消 → 更新变小
- 当梯度小时，**分母小** → 放大效果 → 更新变大

**解决方案**：用**梯度的平方**来衡量"陡峭程度"

### 2.3 RMSprop 算法

**公式**：

```math
S_t = \beta \cdot S_{t-1} + (1 - \beta) \cdot (\nabla_\theta L)^2
```

```math
\theta = \theta - \frac{\eta}{\sqrt{S_t + \epsilon}} \cdot \nabla_\theta L
```

- $S_t$ ：梯度平方的**加权平均**（记录"路况陡峭程度"）
- $\epsilon$ ：防止除零的极小值（如 $10^{-8}$ ）
- $\beta$ ：衰减系数（常取 0.9 或 0.99）

**运作机制**：

| 情况 | 梯度 $\nabla_\theta L$ | $S_t$ （梯度平方） | 实际更新步长 |
| :--- | :--- | :--- | :--- |
| 陡峭方向 | 大 | 大 | $\frac{\eta}{\sqrt{\text{大}}}$ = **小** |
| 平缓方向 | 小 | 小 | $\frac{\eta}{\sqrt{\text{小}}}$ = **大** |

> **效果**：不同方向自动调整步长，收敛更快更稳定

---

## 3. Adam：集大成者

### 3.1 核心思想：结合 Momentum + RMSprop

**分工明确**：
- **Momentum**：管**方向**（利用历史梯度抵消震荡）
- **RMSprop**：管**步长**（自适应调整学习率）

### 3.2 Adam 算法（未修正版本）

**第一步：动量（管方向）**

```math
M_t = \beta_1 \cdot M_{t-1} + (1 - \beta_1) \cdot \nabla_\theta L
```

**第二步：梯度平方（管步长）**

```math
V_t = \beta_2 \cdot V_{t-1} + (1 - \beta_2) \cdot (\nabla_\theta L)^2
```

**第三步：参数更新**

```math
\theta = \theta - \frac{\eta}{\sqrt{V_t} + \epsilon} \cdot M_t
```

- $\beta_1$ ：动量系数（常取 **0.9**）
- $\beta_2$ ：梯度平方系数（常取 **0.999**）

### 3.3 问题：初始化偏差

**现象**：如果 $M_0 = 0, V_0 = 0$ ，则：

```math
M_1 = (1 - \beta_1) \cdot \nabla_\theta L
```

当 $\beta_1 = 0.9$ 时， $M_1 = 0.1 \cdot \nabla_\theta L$ ，**刚开始的更新步子会很小**！

**原因**：初始化为 0 导致前几步被衰减系数严重压制

### 3.4 Adam 完整版（偏差修正）

**偏差修正公式**：

```math
\hat{M}_t = \frac{M_t}{1 - \beta_1^t}
```

```math
\hat{V}_t = \frac{V_t}{1 - \beta_2^t}
```

**完整更新公式**：

```math
\theta = \theta - \frac{\eta}{\sqrt{\hat{V}_t} + \epsilon} \cdot \hat{M}_t
```

**数学直觉**：
- 第 1 步： $1 - \beta_1^1 = 1 - 0.9 = 0.1$ → 分母小，放大 10 倍
- 第 10 步： $1 - 0.9^{10} \approx 0.65$ → 修正减弱
- 第 100 步： $1 - 0.9^{100} \approx 1$ → 几乎无修正

> **效果**：前期放大更新步长，避免"起步太慢"

---

## 4. 算法演进总结

### 4.1 演进路径

```
标准 SGD
    ↓
【问题1】震荡太多，收敛慢
    ↓
Momentum（动量法）
    ↓
【问题2】学习率固定，不同方向不合理
    ↓
RMSprop（自适应学习率）
    ↓
【问题3】能否同时优化方向和步长？
    ↓
Adam（结合 Momentum + RMSprop）
    ↓
【问题4】初始化偏差导致起步慢
    ↓
Adam + 偏差修正（完整版）
```

### 4.2 对比表

| 算法 | 优化内容 | 核心机制 | 主要超参数 |
| :--- | :--- | :--- | :--- |
| **SGD** | 基础方法 | $\theta = \theta - \eta \cdot \nabla L$ | $\eta$ |
| **Momentum** | 优化方向 | 利用历史梯度累积 | $\eta, \beta$ |
| **RMSprop** | 优化步长 | 梯度平方自适应调整 | $\eta, \beta$ |
| **Adam** | 方向+步长 | Momentum + RMSprop | $\eta, \beta_1, \beta_2$ |
| **Adam完整版** | 解决初始偏差 | 加入偏差修正 | $\eta, \beta_1, \beta_2$ |

### 4.3 常用超参数推荐

| 参数 | 推荐值 | 说明 |
| :--- | :--- | :--- |
| $\eta$ （学习率） | 0.001 | 根据任务调整 |
| $\beta_1$ （一阶矩） | 0.9 | 动量衰减系数 |
| $\beta_2$ （二阶矩） | 0.999 | 梯度平方衰减系数 |
| $\epsilon$ | $10^{-8}$ | 防止除零 |

---

## 5. 思考题

### 问题：如果参数不初始化为 0，而是初始化为梯度本身会怎样？

**假设**： $M_0 = \nabla_\theta L_0, V_0 = (\nabla_\theta L_0)^2$ 

**分析**：
- **优点**：避免了初始化偏差，第一步更新幅度合理
- **缺点**：
  1. 第一步的梯度可能不具代表性（随机初始化导致）
  2. 失去了"从零开始积累"的平滑过渡过程
  3. 实际效果不一定比偏差修正更好

> **工程实践**：仍然采用 $M_0 = 0, V_0 = 0$ + 偏差修正，因为：
> - 数学理论完备
> - 稳定性更好
> - 实验证明效果最优

---

## 6. 核心结论

> **深度学习优化的本质**：
> 
> 在**收敛速度**、**稳定性**、**泛化能力**三者之间寻找平衡。
> 
> - **Momentum**：让梯度下降更"聪明"（记住过去的方向）
> - **RMSprop**：让梯度下降更"灵活"（不同方向不同步长）
> - **Adam**：让梯度下降更"完美"（集两者之长）
> 
> **Adam 是目前工业界最主流的优化器**，因为它在绝大多数任务中都能取得良好效果，且超参数调节相对简单。