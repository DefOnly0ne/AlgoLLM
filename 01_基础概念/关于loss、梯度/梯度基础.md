# 学习笔记：梯度下降与参数优化 (Gradient Descent & Optimization)

## 1. 核心目标：寻找最优解

在监督学习（Supervised Learning）中，我们的终极目标是减小 Loss。

* **场景设定**：
    * 输入 $X = \{1, 2, 3\}$
    * 真实标签 $Y = \{3, 6, 9\}$
    * 目标关系：希望模型学习到 $f(x) = 3x$。
* **过程**：我们要调整参数 $\theta$（初始化可能是随机值），通过 Loss Function 衡量差距，最终让 $\theta$ 收敛到 $3$。

---

## 2. 核心机制：为什么导数能“指路”？

当我们把数据代入 Loss 函数后，求导（Derivative）是优化的关键。

### 2.1 导数的物理含义

* **数学视角**：切线的**斜率 (Slope)**。
* **物理视角**：**变化率 (Rate of Change)**。
    * 它告诉我们在当前位置，如果参数 $\theta$ 发生微小变化，Loss 会如何变化。

* **方向指引**：
    * 导数为**正**（上坡）：说明 $\theta$ 变大 $\rightarrow$ Loss 变大。为了减小 Loss，我们要**减小** $\theta$。
    * 导数为**负**（下坡）：说明 $\theta$ 变大 $\rightarrow$ Loss 变小。为了减小 Loss，我们要**增大** $\theta$。

> **结论**：导数不仅是一个数值，它是一个**方向标**。梯度下降法本质上就是**逆着导数的方向走**。

---

## 3. 核心公式：梯度下降法 (Gradient Descent)

$$
\theta \leftarrow \theta - \eta \cdot \text{Gradient}
$$

* **$-$ (减号)**：代表逆向操作（导数指向增长方向，我们要去减少方向）。
* **$\text{Gradient}$ (梯度)**：即 $\frac{\partial Loss}{\partial \theta}$，决定了方向和陡峭程度。
* **$\eta$ (Learning Rate, 学习率)**：决定了这一步迈多大。

---

## 4. 关键超参数：学习率 (Learning Rate, $\eta$)

学习率是人为设置的参数（Hyperparameter），它决定了更新的**步长**。

### 4.1 设置不当的后果

| 状态 | 现象描述 | 后果 |
| :--- | :--- | :--- |
| **过大 (Too Large)** | **梯度震荡 (Oscillation)**<br>步子迈得太大，直接跨过了最低点，到了对面的坡上。 | Loss 曲线来回剧烈震荡，无法收敛，参数更新混乱。 |
| **极大 (Exploding)** | **梯度爆炸 (Gradient Explosion)**<br>步子迈得极远，导致新位置导数更大，下一步迈得更远。 | 导数结果越来越大，数值溢出 (NaN)，模型崩溃。 |
| **过小 (Too Small)** | **收敛缓慢 (Slow Convergence)**<br>像蚂蚁搬家一样，每次只更新一点点。 | Loss 几乎不下降，训练时间极长，且容易陷入局部最优。 |
| **适中 (Good)** | **平滑收敛**<br>开始时步子大（坡陡），接近谷底时步子自动变小（坡缓）。 | 在若干次迭代后高效达到最低 Loss。 |

---

## 5. 进阶概念：从单一到复杂

### 5.1 导数 vs 梯度

* **导数 (Derivative)**：针对**一个参数**（一元函数）的变化率。
* **梯度 (Gradient)**：针对**所有参数**（多元函数）的变化率向量。
    * 它是所有方向中变化最快的那个方向。
    * **深度学习**本质上就是在多维空间中寻找梯度的反方向。

### 5.2 局部最优 (Local Optima)

复杂的神经网络模型，其 Loss 曲面不是一个简单的碗，而是连绵起伏的山脉。

* **问题**：梯度下降法只看脚下。如果走到一个小坑（局部最优），四周都是上坡（梯度为 0），模型会误以为到了终点。
* **解决方案**：
    1. **批次梯度下降 (Batch / Mini-batch)**：利用数据的随机性引入噪音，帮助模型“跳出”小坑。
    2. **Adam 优化器**：
        * **惯性 (Momentum)**：不仅仅看当前的坡度，还考虑之前的速度。即使当前梯度为 0，靠着惯性也能冲出小坑。
        * **自适应学习率**：对不同的参数使用不同的学习率。

---

## 6. 总结

1. **有监督学习**：利用数据（监督者）计算 Loss，指导参数更新。
2. **Loss 计算**：衡量预测与真实的差距（如 MSE、交叉熵）。
3. **求导指路**：利用变化率判断上坡还是下坡。
4. **梯度更新**：$\theta \leftarrow \theta - \eta \cdot \text{Gradient}$。
5. **核心难点**：调节学习率 $\eta$ 以避免震荡或收敛过慢；使用 Adam 等高级优化器解决局部最优问题。