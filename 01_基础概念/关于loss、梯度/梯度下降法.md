# 学习笔记：梯度下降法的三种模式对比

## 核心观点

很多人对全量、随机、批量梯度下降法的理解停留在"数据量大小"的表面，认为：
- 全量 = 用全部数据训练
- 批量 = 用一部分数据训练

但这只是**现象**而非**本质**。真正的区别在于：
- **收敛特性**
- **计算效率**
- **优化稳定性**
- **泛化能力**

**批量大小（Batch Size）是"因"而非"果"** —— 它是为了平衡上述特性而做的工程折中。

---

## 1. 三种梯度下降法的定义

### 1.1 全量梯度下降（Batch Gradient Descent, BGD）
**机制**：每一次参数更新都使用**全部训练数据**计算梯度。

```math
\theta = \theta - \eta \cdot \frac{1}{N} \sum_{i=1}^{N} \nabla L(x_i, \theta)
```

- $N$ ：全部样本数量
- 每次迭代都计算**所有样本导数的均值**

### 1.2 随机梯度下降（Stochastic Gradient Descent, SGD）
**机制**：每次参数更新只随机抽取**一条数据**计算梯度。

```math
\theta = \theta - \eta \cdot \nabla L(x_i, \theta)
```

- 每次迭代只用**单个样本**的梯度

### 1.3 小批量梯度下降（Mini-Batch Gradient Descent, MBGD）
**机制**：每次更新抽取**固定数量（Batch Size）的数据**，用这一批样本梯度的均值更新参数。

```math
\theta = \theta - \eta \cdot \frac{1}{B} \sum_{i=1}^{B} \nabla L(x_i, \theta)
```

- $B$ ：批量大小（如 32、64、128）
- 是前两者的**折中方案**

---

## 2. 核心差异对比

### 2.1 计算效率

| 方法 | 单次迭代速度 | GPU 利用率 | 适用场景 |
| :--- | :--- | :--- | :--- |
| **全量梯度下降** | **极慢**（需遍历全部数据） | 高（可并行计算） | 小数据集（ $\lt$ 10万） |
| **随机梯度下降** | **极快**（只用1条数据） | **极低**（无法并行） | 在线学习、流式数据 |
| **小批量梯度下降** | **适中** | **最优**（充分利用GPU） | **工业界主流** |

> **为什么全量梯度下降不实用？**
> 
> 现代深度学习的数据集动辄百万、千万级别。每次更新都要遍历全部数据，**内存会爆炸，时间成本无法接受**。

> **为什么随机梯度下降也不常用？**
> 
> 单条数据无法构成矩阵运算，**无法发挥 GPU 的并行计算能力**，实际速度反而比小批量更慢。

---

### 2.2 Loss 下降曲线特性

```
全量梯度下降（BGD）：
Loss ↓
  |  ╲
  |   ╲___    平滑、单调下降
  |       ╲___
  +----------→ Epoch

随机梯度下降（SGD）：
Loss ↓
  | ╲ ╱╲  ╱
  |  ╲╱ ╲╱   剧烈震荡、噪声大
  |   ╲ ╱╲
  +----------→ Epoch

小批量梯度下降（MBGD）：
Loss ↓
  | ╲  ╱
  |  ╲╱╲    有波动但整体下降
  |    ╲___
  +----------→ Epoch
```

**解读**：
- **全量**：梯度方向精确，曲线最平滑
- **随机**：梯度噪声极大，路径混乱
- **小批量**：兼具稳定性和探索能力

---

### 2.3 优化稳定性 vs 泛化能力

#### 问题1：鞍点陷阱（Saddle Point）

**全量梯度下降的致命弱点**：

在高维空间中，损失函数常常遇到**鞍点**（梯度为 0 但不是最优点）：

```
        ╱╲        ← 鞍点：某些方向是极小值，
       ╱  ╲          某些方向可继续下降
______╱    ╲______
```

- **全量梯度下降**：梯度精确 → 在鞍点处梯度接近 0 → **误以为到达最优，停止更新**
- **随机梯度下降**：梯度噪声大 → 即使在鞍点也会因为随机扰动**跳出陷阱**，继续探索

#### 问题2：局部最优（Local Minimum）

同理，全量梯度容易陷入**局部最优解**，而随机梯度的"混乱"反而是一种**逃逸能力**。

> **核心启示**：
> 
> **梯度的不准确性有时是巨大优势！**
> 
> - 全量梯度虽然稳定，但容易"困在原地"
> - 随机梯度虽然混乱，但有更强的**全局搜索能力**

---

## 3. 工业界的最佳实践

### 为什么主流方案是小批量梯度下降？

| 维度 | 小批量梯度下降的优势 |
| :--- | :--- |
| **计算效率** | 充分利用 GPU 并行能力（矩阵运算） |
| **内存占用** | 不需要一次加载全部数据 |
| **收敛速度** | 比全量快，比随机稳定 |
| **泛化能力** | 保留适度噪声，避免过拟合 |
| **逃逸鞍点** | 有一定随机性，不易陷入局部最优 |

### 常见 Batch Size 选择

- **小 Batch（16-64）**：噪声大，泛化能力强，但训练不稳定
- **中 Batch（128-256）**：平衡性最好，**工业界主流**
- **大 Batch（512-2048）**：收敛快但容易过拟合，需要特殊优化技巧（如学习率预热）

---

## 4. 总结

| 特性 | 全量梯度下降 | 随机梯度下降 | 小批量梯度下降 |
| :--- | :--- | :--- | :--- |
| **每次更新使用数据量** | 全部 | 1 条 | Batch Size |
| **计算速度** | 慢 | 快 | 适中 |
| **GPU 利用率** | 高 | 极低 | **最优** |
| **Loss 曲线** | 平滑单调 | 剧烈震荡 | 有波动但下降 |
| **收敛稳定性** | **最稳定** | 不稳定 | 较稳定 |
| **逃逸鞍点能力** | **差**（易陷入） | **强**（噪声助力） | 中等 |
| **泛化能力** | 容易过拟合 | **强** | **强** |
| **工业界适用性** | ❌（数据量太大） | ❌（无法并行） | ✅ **主流选择** |

> **核心结论**：
> 
> 批量大小不是简单的"数据多少"问题，而是**计算效率、优化稳定性、泛化能力的三方博弈**。
> 
> 工业界选择小批量梯度下降，是因为它在现代硬件架构（GPU）、大规模数据、复杂损失曲面的约束下，找到了**最优平衡点**。