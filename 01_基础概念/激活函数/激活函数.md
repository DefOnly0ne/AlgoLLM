# 学习笔记：神经网络中的激活函数与非线性演变

## 1. 核心疑问：为什么需要激活函数？为什么不直接使用非线性加权？

### 1.1 线性运算的局限性
在最简单的神经网络中，神经元的计算逻辑是：
$$y = \sum (w_i \cdot x_i) + b$$
这本质上是一个**线性变换**。

* **叠加无效性**：无论神经网络叠加多少层（Layer），如果不引入非线性，多层的线性变换在数学上等价于单层线性变换。
    * 推导：$y = W_2(W_1 x) = (W_2 W_1)x = W_{new} x$。
    * 结论：没有激活函数，深层网络与单层感知机没有区别，无法拟合复杂函数。

### 1.2 为什么不直接把 $Wx+b$ 改成非线性函数（如 $Wx^2$）？
理论上可行，但在工程和训练上存在巨大障碍：

1.  **梯度/数值爆炸 (Exploding Gradient)**：
    * 假如使用 $f(x) = x^2$ 作为核心函数，求导后为 $2x$。
    * 在深层网络（如 10 层）的反向传播中，梯度会连乘：$2x \cdot 2x \dots$。如果 $x$ 稍大，数值会瞬间呈指数级爆炸，导致模型无法收敛。
    * 同时，平方运算会破坏符号信息（-5 和 +5 变得一样），丢失方向性。
2.  **计算效率**：
    * 现代深度学习依赖 GPU 进行大规模**矩阵乘法**。线性运算（$Wx+b$）是计算机最擅长、优化最极致的操作。
    * 如果引入复杂的高阶非线性函数，将无法利用矩阵并行的优势，训练速度会慢几个数量级。
3.  **优化难度**：
    * 非线性权重的损失函数曲面极度不平滑（非凸），充满了局部最优解，梯度下降法很难找到全局最优。

> **最佳实践**：采用 **“线性变换 (矩阵乘法) + 非线性激活 (Activation Function)”** 的组合。即 $y = \sigma(Wx+b)$。这种“分段线性”的方法既利用了硬件优势，又具备了万能拟合能力。

---

## 2. 激活函数的演变史

### 第一阶段：Sigmoid / Tanh (早期模拟生物)

**公式**：$\sigma(x) = \frac{1}{1+e^{-x}}$

* **特点**：
    * 将输入压缩到 $(0, 1)$ 区间。
    * **天然的概率解释**：输出越接近 1，代表神经元越“兴奋”。
* **优点**：
    * 平滑、可导。
    * 防止输出值无限发散（数值稳定性）。
* **致命缺点**：
    1.  **梯度消失 (Gradient Vanishing)**：Sigmoid 的导数最大值仅为 **0.25**。在反向传播链式法则中，每一层都要乘这个小于 0.25 的数。层数一多，传到输入层的梯度趋近于 0，导致参数无法更新。
    2.  **计算昂贵**：$e^{-x}$ 是幂运算，比加减乘除耗时。
    3.  **非零中心化**：输出恒大于 0，导致收敛速度变慢（Zigzag path）。

### 第二阶段：ReLU (修正线性单元) —— 深度学习的转折点

**公式**：$f(x) = \max(0, x)$

* **特点**：
    * $x > 0$ 时，输出 $x$；$x \le 0$ 时，输出 $0$。
* **为什么更有效？**
    1.  **解决梯度消失**：在 $x>0$ 的区域，导数恒为 **1**。梯度可以无损地穿透无数层网络，使得深层网络（Deep Learning）的训练成为可能。
    2.  **计算极快**：只需要判断正负，无复杂的数学运算。
    3.  **稀疏激活性 (Sparsity)**：
        * **机制**：负数区域强制截断为 0，意味着在任何时刻，网络中只有一部分神经元在工作（“醒着”）。
        * **意义**：这模拟了人脑的节能机制（稀疏编码）。相比 Sigmoid 的“全员激活”（稠密表达），稀疏性更有利于提取关键特征，鲁棒性更强。
* **缺点**：
    * **Dead ReLU (神经元死亡)**：如果某次更新导致权重变为很大的负数，该神经元再也无法被激活（输出恒为 0，梯度恒为 0）。这个节点就“死”了，参数永远不再更新。

### 第三阶段：Leaky ReLU / GELU (现代优化)

针对 ReLU 的“死亡”问题进行的改进。

1.  **Leaky ReLU**：
    * 在 $x < 0$ 时，不直接置 0，而是给一个很小的斜率（如 $0.01x$）。
    * **作用**：保留了微弱的梯度，给负区间的神经元“一点机会”，防止彻底死亡。

2.  **GELU (Gaussian Error Linear Unit)**：
    * **应用**：BERT、GPT 等大模型的标配。
    * **特点**：在 0 附近不是生硬的折角，而是平滑的曲线。结合了 ReLU 的性质和概率分布的思想。

---

## 3. 总结与对比表

| 特性 | Sigmoid | ReLU | Leaky ReLU / GELU |
| :--- | :--- | :--- | :--- |
| **非线性** | 是 | 是 | 是 |
| **梯度消失问题** | **严重** (最大梯度0.25) | **解决** (正区间梯度为1) | **解决** |
| **计算复杂度** | 高 (幂运算) | **极低** (比较运算) | 低 / 中 |
| **输出范围** | (0, 1) | [0, +∞) | (-∞, +∞) |
| **稀疏性** | 无 (稠密，全员激活) | **有** (负区间为0) | 部分保留 |
| **主要缺点** | 梯度消失、计算慢 | Dead ReLU (神经元坏死) | 计算稍复杂 (GELU) |

> **核心结论**：
> 深度学习的本质是在**计算效率**与**拟合能力**之间寻找平衡。
> 
> * **线性运算**负责“切分空间”，利用 GPU 加速；
> * **激活函数**负责“扭曲空间”，赋予模型灵魂。
> 
> 目前最主流的选择依然是 **ReLU** 及其变体，因为它们最完美地平衡了**梯度传播效率**和**稀疏表达能力**。