# 学习笔记：Transformer 中的位置编码（Positional Encoding）

## 0. 核心问题：为什么需要位置编码？

### 0.1 注意力机制的"位置盲"特性

在计算注意力时，**每个 token 都要和其他所有 token 计算缩放点积**：

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
```

**问题**：这个计算过程**完全不在乎各个 token 的位置**！

**举例**：
- 输入序列："我 爱 你" → `[1, 2, 3]`
- 打乱顺序："你 爱 我" → `[3, 2, 1]`

如果不加位置信息，模型会认为这两个句子**完全一样**，因为注意力矩阵的计算结果相同。

> **结论**：必须加上位置信息，告诉模型"谁在前，谁在后"。

---

## 1. 为什么不直接用 0, 1, 2, 3... 作为位置编码？

### 1.1 方案1：简单递增（朴素方法）

**想法**：直接把位置索引加到词向量上
- 第1个词：词向量 + 1
- 第2个词：词向量 + 2
- 第3个词：词向量 + 3
- ...

### 1.2 致命缺陷

#### 问题1：数值爆炸破坏方差

- 第 1 个词加 1
- 第 512 个词加 512

**后果**：
- 词向量的值通常在 `[-1, 1]` 范围内
- 位置信息的值却高达几百，**完全淹没了词向量的语义信息**
- 破坏了词向量的分布，导致**模型训练不稳定**

#### 问题2：无法泛化到更长序列

**训练时**：数据集中所有句子长度 ≤ 512
- 模型只见过位置编码 1, 2, 3, ..., 512

**推理时**：用户输入了长度为 1000 的文本
- 位置编码 513, 514, ..., 1000 是**从未见过的新值**
- 模型的**泛化能力极差**，无法理解这些位置

> **核心矛盾**：简单递增的位置编码既会破坏数值稳定性，又无法扩展到训练时未见过的长度。

---

## 2. 正余弦位置编码（Sinusoidal Positional Encoding）

### 2.1 公式定义

对于位置 $\text{pos}$ 和维度 $i$ ：

**偶数维度**：

```math
PE_{(\text{pos}, 2i)} = \sin\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)
```

**奇数维度**：

```math
PE_{(\text{pos}, 2i+1)} = \cos\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)
```

- $\text{pos}$ ：token 在序列中的位置（0, 1, 2, ...）
- $i$ ：词向量的维度索引（0, 1, 2, ..., $d_{\text{model}}/2$ ）
- $d_{\text{model}}$ ：词向量的总维度（如 512）

### 2.2 为什么有两个公式？偶数项和奇数项交替计算

**设计目的**：让模型自然地捕捉**相对位置信息**

#### 核心原理：三角函数和角公式

对于任意位置 $\text{pos}$ 和偏移量 $k$ ：

```math
\sin(\text{pos} + k) = \sin(\text{pos})\cos(k) + \cos(\text{pos})\sin(k)
```

```math
\cos(\text{pos} + k) = \cos(\text{pos})\cos(k) - \sin(\text{pos})\sin(k)
```

**改写为矩阵形式**：

```math
\begin{bmatrix}
PE_{(\text{pos}+k, 2i)} \\
PE_{(\text{pos}+k, 2i+1)}
\end{bmatrix}
=
\begin{bmatrix}
\cos(k\theta) & \sin(k\theta) \\
-\sin(k\theta) & \cos(k\theta)
\end{bmatrix}
\begin{bmatrix}
PE_{(\text{pos}, 2i)} \\
PE_{(\text{pos}, 2i+1)}
\end{bmatrix}
```

其中 $\theta = \frac{1}{10000^{2i/d_{\text{model}}}}$ 

**关键结论**：
> **位置 $\text{pos} + k$ 的编码可以通过位置 $\text{pos}$ 的编码进行固定的线性变换得到！**

### 2.3 为什么这很重要？

#### 与注意力机制的天然契合

注意力机制本质上就是**线性变换** + softmax：

```math
\text{Attention} = \text{softmax}(QK^T)V
```

- $Q, K, V$ 都是通过**线性变换**（矩阵乘法）得到的

**模型可以做什么**：
- 通过调整 $W_Q, W_K, W_V$ 的权重
- **自动学习并利用相对位置关系**
- 比如："动词通常在名词后面"、"形容词通常在名词前面"

> **这意味着模型不需要显式地学习绝对位置，而是能够通过线性变换直接学习相对位置！**

---

## 3. 正余弦位置编码的两大优势

### 3.1 优势1：表达多尺度位置信息

#### 低维度 → 局部位置信息（高频）

- **周期短**：变化快
- **相邻词之间的区别大**
- 适合捕捉**短程依赖**（前一个词、后一个词）

```
维度 0-64：
词1: sin(0.01)    词2: sin(0.02)    词3: sin(0.03)
      ↑ 差距大 ↑          ↑ 差距大 ↑
```

#### 高维度 → 全局位置信息（低频）

- **周期长**：变化慢
- **相邻词之间的区别小**
- 适合捕捉**长程依赖**（前一段话的结尾、后一段话的开头）

```
维度 448-512：
词1: sin(0.0001)  词100: sin(0.01)  词500: sin(0.05)
      ↑ 差距小 ↑            ↑ 差距小 ↑
```

**直观理解**：

| 维度范围 | 波长 | 捕捉信息 | 类比 |
| :--- | :--- | :--- | :--- |
| 低维（0-128） | 短 | 邻近词的顺序 | 显微镜（看细节） |
| 中维（128-384） | 中 | 句子内的结构 | 放大镜（看句子） |
| 高维（384-512） | 长 | 段落间的关系 | 望远镜（看全局） |

> **一个词的位置编码同时包含了局部和全局的位置信息！**

### 3.2 优势2：天然扩展到更长序列

#### 三角函数的关键特性

1. **连续性**：对于任意位置 $\text{pos}$ ，都能计算出有意义的值
2. **周期性**：位置之间的相对关系保持一致
3. **光滑性**：相邻位置的编码平滑过渡

**训练与推理的对比**：

| 阶段 | 序列长度 | 位置编码 | 模型理解 |
| :--- | :--- | :--- | :--- |
| 训练 | ≤ 512 | 见过 0-512 的所有位置变化 | 学习位置模式 |
| 推理 | 1000 | 位置 513-1000 是**新位置** | 三角函数保证平滑延续 |

**为什么模型能理解超出训练长度的位置？**

- 训练时，模型见过**无数次**位置 1→2、2→3、510→511 的变化
- 这些变化的**模式**（通过三角函数的周期性）是一致的
- 推理时，位置 512→513、999→1000 遵循**同样的模式**
- 模型能够**泛化**理解这些新位置

**数学直觉**：

```
训练见过：
sin(1) → sin(2) → sin(3) → ... → sin(512)
△ = 固定的相对变化

推理时：
sin(513) → sin(514) → sin(515)
△ = 相同的相对变化（三角函数保证）
```

---

## 4. 可视化理解

### 4.1 不同维度的位置编码波形

```
低维度（高频）：
  ┌─┐ ┌─┐ ┌─┐ ┌─┐     波长短，变化快
  │ │ │ │ │ │ │ │     相邻位置差异大
──┘ └─┘ └─┘ └─┘ └──

中维度（中频）：
  ┌───┐   ┌───┐       波长中等
  │   │   │   │       
──┘   └───┘   └────

高维度（低频）：
  ┌─────────┐         波长长，变化慢
  │         │         相邻位置差异小
──┘         └────────
```

### 4.2 位置编码的维度分布

```
词向量维度 d_model = 512

维度   0-127  |  局部信息（词级别）
        ↓     
维度 128-383  |  中程信息（句子级别）
        ↓     
维度 384-511  |  全局信息（段落级别）
```

---

## 5. 总结

### 5.1 正余弦位置编码的三大优势

| 优势 | 简单递增编码 | 正余弦编码 |
| :--- | :--- | :--- |
| **数值稳定性** | ❌ 值随位置爆炸 | ✅ 值在 `[-1, 1]` 范围 |
| **相对位置学习** | ❌ 只能学绝对位置 | ✅ 线性变换自然表达 |
| **多尺度信息** | ❌ 单一尺度 | ✅ 同时表达局部+全局 |
| **长度泛化** | ❌ 无法扩展到新长度 | ✅ 三角函数天然延续 |

### 5.2 核心公式回顾

```math
PE_{(\text{pos}, 2i)} = \sin\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)
```

```math
PE_{(\text{pos}, 2i+1)} = \cos\left(\frac{\text{pos}}{10000^{2i/d_{\text{model}}}}\right)
```

**关键参数**：
- $10000$ ：控制波长的基数（经验值，效果最好）
- 指数项 $2i/d_{\text{model}}$ ：让不同维度有不同的波长

### 5.3 位置编码的本质

> **位置编码不是简单的"给每个词加个序号"，而是：**
> 
> 1. 用**多频率的波**编码位置信息
> 2. 让模型通过**线性变换**学习相对位置
> 3. 同时捕捉**局部和全局**的位置关系
> 4. 实现对**未见长度**的自然泛化
> 
> **这是 Transformer 能够理解序列结构的数学基础！**

---

## 6. 扩展思考

### 问题：为什么不用可学习的位置编码？

**可学习位置编码**：把位置编码当作模型参数，通过训练学习

**对比**：

| 方案 | 优点 | 缺点 |
| :--- | :--- | :--- |
| **正余弦编码** | 无需训练、天然泛化 | 表达能力可能不如可学习 |
| **可学习编码** | 表达能力更强、适配任务 | 无法泛化到训练时未见的长度 |

**实践**：
- **原始 Transformer**：使用正余弦编码
- **BERT**：使用可学习位置编码（因为有最大长度限制 512）
- **GPT**：使用可学习位置编码
- **RoPE（旋转位置编码）**：正余弦编码的改进版，用于长文本模型

> **工程选择**：如果任务有固定的最大长度，可学习编码通常效果更好；如果需要处理任意长度，正余弦编码更稳定。