### 第一代：协同过滤时代 (Pre-2010)

**核心思想：找“替身”或者找“兄弟”**
**场景：** 早期亚马逊卖书，或者 Netflix 租碟。

这时候没有复杂的神经网络，计算机算力也有限。推荐系统非常直观。

#### 1. 怎么做？

* **User-based CF（找替身）：**
* **逻辑：** 我发现用户 A 和用户 B 历史买过的书重合度高达 90%（都买了《哈利波特》和《魔戒》）。那么，当 A 买了一本新书《冰与火之歌》时，我就猜 B 也会喜欢，于是推给 B。
* **缺点：** 用户量一旦爆炸（几亿人），算“谁和谁相似”这笔账算不过来，矩阵太大了。


* **Item-based CF（找兄弟）：**
* **逻辑（亚马逊的创举）：** 我不管人，我看商品。大数据显示，买《尿布》的人，大概率也会买《啤酒》。那我就把这两个商品绑定。你加购了尿布，我就推啤酒。
* **优点：** 商品比人少，计算量可控，且更稳定。



#### 2. 这一代的痛点

**“冷启动”和“稀疏性”。**
如果是新用户，没买过东西，系统就瞎了（没法算相似度）。如果是新书，没人买过，也永远推不出去。

---

### 第二代：逻辑回归与特征工程时代 (2010 - 2015)

**核心思想：把一切都变成“特征”来打分**
**场景：** 门户网站广告、早期今日头条。

为了解决冷启动，工程师开始想：**即使你没买过东西，我也知道你的属性（男、25岁、北京）**。能不能利用这些？

#### 1. 怎么做？(Logistic Regression - LR)

这一代的核心是**CTR 预估（点击率预测）**。
系统把推荐变成了一个**数学公式**：


* **具体例子：**
* **特征工程：** 工程师人工提取特征。比如：用户是男性(1)，当前是晚上10点(1)，文章是军事类(1)。
* **交叉特征（核心魔法）：** 工程师发现单独看“男性”没用，单独看“晚上”也没用。但是**“男性 + 晚上 + 游戏广告”**这个组合非常容易转化。于是人工把这三个拼成一个强特征。


* **Facebook 的 GBDT+LR (2014)：**
* 人工组合特征太累了，Facebook 提出用 GBDT（树模型）自动去发现路径（比如：如果年龄<30 且 喜欢科技 -> 则是目标人群），然后把树的结果喂给 LR 模型。这成为了当时的业界标配。



#### 2. 这一代的痛点

**“泛化能力差”和“人工太累”。**
LR 模型本质上是**记忆**。它只能记住“过去发生过的组合”。如果来了一个没见过的组合（比如“喜欢滑板的程序员”），它就不知道咋办了。而且人工挖掘特征（比如啤酒+尿布）效率太低。

---

### 第三代：深度学习与 Embedding 时代 (2016 - 2018)

**核心思想：万物皆向量，让模型学会“举一反三”**
**场景：** Google Play 应用商店，Youtube 视频推荐。

这是分水岭。2016年 Google 发表了 **Wide & Deep**，彻底改变了游戏规则。

#### 1. 核心技术：Embedding

计算机不认识“耐克鞋”和“阿迪鞋”这两个词。
深度学习通过 **Embedding** 把它们变成两个向量（一串数字）。在数学空间里，这两个向量靠得很近。
**具体例子：** 即使系统从未见过你买“阿迪”，但因为它知道你买过“耐克”，而“耐克”和“阿迪”在向量空间里是邻居，系统就能通过**泛化**推算出你可能喜欢阿迪。

#### 2. 里程碑模型：Wide & Deep (2016)

Google 发现，推荐既要**记性好**，又要**脑子活**。

* **Wide 部分（记性好）：** 就像老一代模型。记住“买过 iPhone 14 的人通常会买 iPhone 14 手机壳”。这是强规则，不能错。
* **Deep 部分（脑子活）：** 用神经网络去“猜”。发现“喜欢科幻电影”的人，和“喜欢硬核游戏”的人，虽然表面没关系，但背后有某种隐形联系。
* **结果：** 既保证了精准（Wide），又有了惊喜感（Deep）。

#### 3. 里程碑模型：DeepFM (2017)

Wide&Deep 里的 Wide 部分还得人工做一点特征。华为提出的 DeepFM 说：**我全都要自动的**。它用 FM（因子分解机）替换了 Wide 部分，实现了从头到尾不需要人工干预特征，非常适合懒人工程师，效果还贼好。

---

### 第四代：注意力机制与序列化时代 (2018 - 2020)

**核心思想：此时此刻，你只在乎特定的历史**
**场景：** 淘宝首页猜你喜欢，抖音无限流。

阿里在这个阶段贡献最大。因为电商场景下，人的兴趣是**多变且实时**的。

#### 1. 痛点：历史信息的“平均主义”

以前的模型会把你过去一年的行为算一个平均值。
**具体例子：**
你上周买了个“哑铃”（健身兴趣），昨天给女朋友买了个“口红”（美妆兴趣），今天你在看“蛋白粉”。
如果是老模型，它会把你定义为“一个喜欢健身又喜欢美妆的怪人”，然后推出来的东西不伦不类。

#### 2. 解决方案：DIN (Deep Interest Network, 2018)

阿里引入了 **Attention（注意力机制）**。

* **逻辑：** 当系统准备给你推“蛋白粉”时，模型回头看你的历史记录。
* **Attention 运作：** 模型发现历史记录里的“哑铃”和当前的“蛋白粉”高度相关，于是给“哑铃”极高的权重；而发现“口红”和“蛋白粉”没关系，于是自动忽略“口红”。
* **结果：** 在推蛋白粉时，你就是一个纯粹的健身爱好者；在推粉底液时，你就是一个纯粹的美妆买家。**千人千面进化成了“一人千面”。**

---

### 第五代：多目标与因果推断 (2020 - 至今)

**核心思想：既要点击，又要时长，还要点赞**
**场景：** 抖音/TikTok、快手。

现在的 APP 不止想让你点进去（CTR），还想让你看完（完播率），甚至想让你下单（CVR）。

#### 1. 痛点：目标的冲突

**具体例子（标题党）：**
如果不加控制，模型会疯狂给你推“震惊！某男子竟然...”这种视频。你会狂点（CTR高），但进去发现被骗了马上退出（完播率低）。这就损害了用户体验。

#### 2. 解决方案：MMoE (Multi-gate Mixture-of-Experts)

Google 提出的 MMoE 就像是雇佣了多个“专家”大脑。

* **专家 A：** 专门研究什么样的图容易被点击。
* **专家 B：** 专门研究什么样的内容让人愿意看完。
* **门控机制 (Gate)：** 针对不同的任务，分配不同的专家权重。
* **最终输出：** `总分 = w1 * 点击分 + w2 * 完播分 + w3 * 点赞分`。
通过调整 w1, w2, w3，像抖音这种 App 就能精确控制它是想要更多的人点进来，还是想要更多人沉浸看下去。

---

### 总结：从“猜你要啥”到“懂你所想”

| 阶段 | 核心技术 | 典型公司/论文 | 解决的核心问题 | 比喻 |
| --- | --- | --- | --- | --- |
| **第一代** | 协同过滤 (CF) | Amazon / Netflix | 还没数据时怎么推 | **找替身**：你也喜欢周杰伦？那我们也喜欢一样的东西。 |
| **第二代** | 逻辑回归 (LR) | Facebook / Google | 大规模特征处理 | **算公式**：男+晚上+游戏=点击。 |
| **第三代** | Wide & Deep | Google / Huawei | 记忆与泛化的平衡 | **左右互搏**：左手守正（规则），右手出奇（联想）。 |
| **第四代** | Attention (DIN) | Alibaba (淘宝) | 兴趣是动态变化的 | **划重点**：为了买锅，我只关心我看过的铲子，不关心看过的衣服。 |
| **第五代** | 多目标 (MMoE) | TikTok / Google | 点击、转化、时长都要 | **端水大师**：既要标题吸引人，又要内容留得住人。 |

